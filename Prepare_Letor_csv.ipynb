{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b856df7f-5578-4b9c-b61c-91b2800e62a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting to process in chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_542225/4243773781.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  for chunk in pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2,263,154 rows out of 2,263,154...\n",
      "Processing complete.\n",
      "CSV saved as: Letor_train_fold5.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Folder containing the text files\n",
    "data_folder = \"Fold5/\"\n",
    "train_file = \"train.txt\"\n",
    "output_csv = \"Letor_train_fold5.csv\"  # Output CSV file\n",
    "\n",
    "# LETOR datasets typically have 136 features + 2 extra columns (label, query ID)\n",
    "num_features = 136\n",
    "column_names = [\"relevance\", \"query_id\"] + [f\"feature_{i}\" for i in range(1, num_features + 1)]\n",
    "\n",
    "# Define chunk size and maximum rows to process\n",
    "chunk_size = 50000   \n",
    "nrows = 3_000_000  # Stop processing after nrows (Put > 2,263,154M if you want to read entire training set)\n",
    "processed_rows = 0\n",
    "\n",
    "# Open CSV file to write in chunks\n",
    "with open(output_csv, \"w\") as output_file:\n",
    "    # Write header\n",
    "    output_file.write(\",\".join(column_names) + \"\\n\")\n",
    "\n",
    "    # Read the train file in chunks\n",
    "    print(\"\\nStarting to process in chunks...\")\n",
    "    for chunk in pd.read_csv(\n",
    "        os.path.join(data_folder, train_file),\n",
    "        names=column_names,\n",
    "        delim_whitespace=True,\n",
    "        dtype=str,  # Read everything as string initially\n",
    "        low_memory=False,\n",
    "        chunksize=chunk_size,  # Read in chunks\n",
    "    ):\n",
    "        # Remove \"qid:\" prefix from `query_id`\n",
    "        chunk[\"query_id\"] = chunk[\"query_id\"].str.replace(\"qid:\", \"\", regex=True).astype(int)\n",
    "\n",
    "        # Remove feature index prefixes (e.g., \"7:0.5\" → \"0.5\")\n",
    "        for col in column_names[2:]:  # Skip relevance & query_id\n",
    "            chunk[col] = chunk[col].str.split(\":\").str[1].astype(float)\n",
    "\n",
    "        # Append chunk to CSV\n",
    "        chunk.to_csv(output_file, mode=\"a\", index=False, header=False)\n",
    "\n",
    "        # Update processed rows count\n",
    "        processed_rows += len(chunk)\n",
    "\n",
    "        # Print real-time progress\n",
    "        sys.stdout.write(f\"\\rProcessed {processed_rows:,} rows out of 2,263,154...\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Stop processing if we exceed max_rows\n",
    "        if processed_rows >= nrows:\n",
    "            print(\"\\nReached nrows limit. Stopping early.\")\n",
    "            break\n",
    "\n",
    "# Final check\n",
    "print(\"\\nProcessing complete.\")\n",
    "print(f\"CSV saved as: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29fa3296-e712-41be-8c72-7b21eed30697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting to process validation data in chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_542225/867423584.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  for chunk in pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 760,753 rows out of 760,752...\n",
      "Processing complete.\n",
      "Validation CSV saved as: Letor_vali_fold5.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Folder containing the text files\n",
    "data_folder = \"Fold5/\"\n",
    "vali_file = \"vali.txt\"\n",
    "output_csv = \"Letor_vali_fold5.csv\"  # Output CSV file\n",
    "\n",
    "# LETOR datasets typically have 136 features + 2 extra columns (label, query ID)\n",
    "num_features = 136\n",
    "column_names = [\"relevance\", \"query_id\"] + [f\"feature_{i}\" for i in range(1, num_features + 1)]\n",
    "\n",
    "# Define chunk size and maximum rows to process\n",
    "chunk_size = 50000   \n",
    "nrows = 1_000_000  # Stop processing after nrows (Set > 760,752 to read the entire validation set)\n",
    "processed_rows = 0\n",
    "\n",
    "# Open CSV file to write in chunks\n",
    "with open(output_csv, \"w\") as output_file:\n",
    "    # Write header\n",
    "    output_file.write(\",\".join(column_names) + \"\\n\")\n",
    "\n",
    "    # Read the validation file in chunks\n",
    "    print(\"\\nStarting to process validation data in chunks...\")\n",
    "    for chunk in pd.read_csv(\n",
    "        os.path.join(data_folder, vali_file),\n",
    "        names=column_names,\n",
    "        delim_whitespace=True,\n",
    "        dtype=str,  # Read everything as string initially\n",
    "        low_memory=False,\n",
    "        chunksize=chunk_size,  # Read in chunks\n",
    "    ):\n",
    "        # Remove \"qid:\" prefix from `query_id`\n",
    "        chunk[\"query_id\"] = chunk[\"query_id\"].str.replace(\"qid:\", \"\", regex=True).astype(int)\n",
    "\n",
    "        # Remove feature index prefixes (e.g., \"7:0.5\" → \"0.5\")\n",
    "        for col in column_names[2:]:  # Skip relevance & query_id\n",
    "            chunk[col] = chunk[col].str.split(\":\").str[1].astype(float)\n",
    "\n",
    "        # Append chunk to CSV\n",
    "        chunk.to_csv(output_file, mode=\"a\", index=False, header=False)\n",
    "\n",
    "        # Update processed rows count\n",
    "        processed_rows += len(chunk)\n",
    "\n",
    "        # Print real-time progress\n",
    "        sys.stdout.write(f\"\\rProcessed {processed_rows:,} rows out of 760,752...\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Stop processing if we exceed max_rows\n",
    "        if processed_rows >= nrows:\n",
    "            print(\"\\nReached nrows limit. Stopping early.\")\n",
    "            break\n",
    "\n",
    "# Final check\n",
    "print(\"\\nProcessing complete.\")\n",
    "print(f\"Validation CSV saved as: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f62296a-e235-48d7-83b1-62593f24d951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting to process test data in chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_544861/1779297140.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  for chunk in pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 747,218 rows out of 747,217...\n",
      "Processing complete.\n",
      "Test CSV saved as: Letor_test_fold5.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Folder containing the text files\n",
    "data_folder = \"Fold5/\"\n",
    "test_file = \"test.txt\"\n",
    "output_csv = \"Letor_test_fold5.csv\"  # Output CSV file\n",
    "\n",
    "# LETOR datasets typically have 136 features + 2 extra columns (label, query ID)\n",
    "num_features = 136\n",
    "column_names = [\"relevance\", \"query_id\"] + [f\"feature_{i}\" for i in range(1, num_features + 1)]\n",
    "\n",
    "# Define chunk size and maximum rows to process\n",
    "chunk_size = 50000   \n",
    "nrows = 1_000_000  # Stop processing after nrows (Set > 747,217 to read the entire test set)\n",
    "processed_rows = 0\n",
    "\n",
    "# Open CSV file to write in chunks\n",
    "with open(output_csv, \"w\") as output_file:\n",
    "    # Write header\n",
    "    output_file.write(\",\".join(column_names) + \"\\n\")\n",
    "\n",
    "    # Read the test file in chunks\n",
    "    print(\"\\nStarting to process test data in chunks...\")\n",
    "    for chunk in pd.read_csv(\n",
    "        os.path.join(data_folder, test_file),\n",
    "        names=column_names,\n",
    "        delim_whitespace=True,\n",
    "        dtype=str,  # Read everything as string initially\n",
    "        low_memory=False,\n",
    "        chunksize=chunk_size,  # Read in chunks\n",
    "    ):\n",
    "        # Remove \"qid:\" prefix from `query_id`\n",
    "        chunk[\"query_id\"] = chunk[\"query_id\"].str.replace(\"qid:\", \"\", regex=True).astype(int)\n",
    "\n",
    "        # Remove feature index prefixes (e.g., \"7:0.5\" → \"0.5\")\n",
    "        for col in column_names[2:]:  # Skip relevance & query_id\n",
    "            chunk[col] = chunk[col].str.split(\":\").str[1].astype(float)\n",
    "\n",
    "        # Append chunk to CSV\n",
    "        chunk.to_csv(output_file, mode=\"a\", index=False, header=False)\n",
    "\n",
    "        # Update processed rows count\n",
    "        processed_rows += len(chunk)\n",
    "\n",
    "        # Print real-time progress\n",
    "        sys.stdout.write(f\"\\rProcessed {processed_rows:,} rows out of 747,217...\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Stop processing if we exceed max_rows\n",
    "        if processed_rows >= nrows:\n",
    "            print(\"\\nReached nrows limit. Stopping early.\")\n",
    "            break\n",
    "\n",
    "# Final check\n",
    "print(\"\\nProcessing complete.\")\n",
    "print(f\"Test CSV saved as: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5694cd02-a72a-48af-9f57-7f942ca309de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (IR Project SSH v.0.2)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
